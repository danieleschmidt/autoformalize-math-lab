# Performance Testing Workflow for autoformalize-math-lab
# Copy to .github/workflows/performance.yml to enable

name: Performance Testing

on:
  schedule:
    - cron: '0 6 * * 1'  # Weekly on Monday at 6 AM UTC
  push:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'tests/performance/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'src/**'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test to run'
        required: true
        default: 'basic'
        type: choice
        options:
          - basic
          - comprehensive
          - load
          - memory
      duration_minutes:
        description: 'Test duration in minutes'
        required: false
        default: '10'
        type: string

env:
  PYTHON_VERSION: "3.11"

jobs:
  basic-benchmarks:
    name: Basic Performance Benchmarks
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'basic' || github.event.inputs.test_type == '' || github.event_name != 'workflow_dispatch'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark memory-profiler psutil
        
    - name: Run basic benchmarks
      run: |
        pytest tests/performance/ -v \
          --benchmark-only \
          --benchmark-json=benchmark-results.json \
          --benchmark-min-rounds=3 \
          --benchmark-max-time=300
          
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-basic
        path: benchmark-results.json
        
    - name: Compare with baseline
      run: |
        # Download previous benchmark results if available
        if [ -f baseline-benchmarks.json ]; then
          python scripts/compare_benchmarks.py \
            baseline-benchmarks.json \
            benchmark-results.json \
            --threshold=10  # 10% performance regression threshold
        fi

  comprehensive-performance:
    name: Comprehensive Performance Testing
    runs-on: ubuntu-latest
    timeout-minutes: 60
    if: github.event.inputs.test_type == 'comprehensive' || github.event_name == 'schedule'
    
    strategy:
      matrix:
        test_category: [parsing, generation, verification, end_to_end]
        
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark memory-profiler line-profiler
        
    - name: Run comprehensive tests - ${{ matrix.test_category }}
      run: |
        pytest tests/performance/test_${{ matrix.test_category }}.py -v \
          --benchmark-only \
          --benchmark-json=benchmark-${{ matrix.test_category }}.json \
          --benchmark-min-rounds=5 \
          --benchmark-max-time=600
          
    - name: Memory profiling - ${{ matrix.test_category }}
      run: |
        python -m memory_profiler scripts/profile_${{ matrix.test_category }}.py > memory-profile-${{ matrix.test_category }}.txt
        
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results-${{ matrix.test_category }}
        path: |
          benchmark-${{ matrix.test_category }}.json
          memory-profile-${{ matrix.test_category }}.txt

  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    timeout-minutes: 90
    if: github.event.inputs.test_type == 'load' || github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install locust pytest-xdist
        
    - name: Start application server
      run: |
        # Start the autoformalize server in background
        python -m autoformalize.server --port 8080 &
        sleep 10  # Wait for server to start
        
    - name: Run load tests
      run: |
        DURATION=${{ github.event.inputs.duration_minutes || '10' }}
        
        # Run Locust load tests
        locust -f tests/performance/locustfile.py \
          --host=http://localhost:8080 \
          --users=10 \
          --spawn-rate=2 \
          --run-time=${DURATION}m \
          --html=load-test-report.html \
          --csv=load-test-results
          
    - name: Run concurrent processing tests
      run: |
        # Test concurrent formalization requests
        pytest tests/performance/test_concurrent.py -v \
          --workers=auto \
          --benchmark-json=concurrent-benchmark.json
          
    - name: Upload load test results
      uses: actions/upload-artifact@v3
      with:
        name: load-test-results
        path: |
          load-test-report.html
          load-test-results*.csv
          concurrent-benchmark.json

  memory-profiling:
    name: Memory Usage Analysis
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'memory' || github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install memory-profiler pympler tracemalloc-tools
        
    - name: Run memory leak detection
      run: |
        python tests/performance/memory_leak_test.py > memory-leak-results.txt
        
    - name: Profile memory usage patterns
      run: |
        python -m memory_profiler tests/performance/memory_profile_script.py > memory-usage-profile.txt
        
    - name: Analyze memory allocation
      run: |
        python tests/performance/memory_allocation_analysis.py > memory-allocation-report.txt
        
    - name: Upload memory analysis results
      uses: actions/upload-artifact@v3
      with:
        name: memory-analysis
        path: |
          memory-leak-results.txt
          memory-usage-profile.txt
          memory-allocation-report.txt

  llm-performance:
    name: LLM API Performance Testing
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'comprehensive'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install aiohttp asyncio pytest-asyncio
        
    - name: Test LLM API response times
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_TEST_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_TEST_API_KEY }}
      run: |
        pytest tests/performance/test_llm_performance.py -v \
          --benchmark-json=llm-benchmark.json \
          -m "not expensive"
          
    - name: Test API rate limiting behavior
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_TEST_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_TEST_API_KEY }}
      run: |
        python tests/performance/rate_limit_test.py > rate-limit-results.txt
        
    - name: Upload LLM performance results
      uses: actions/upload-artifact@v3
      with:
        name: llm-performance-results
        path: |
          llm-benchmark.json
          rate-limit-results.txt

  proof-assistant-performance:
    name: Proof Assistant Performance
    runs-on: ubuntu-latest
    timeout-minutes: 60
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'comprehensive'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install Lean 4
      run: |
        curl -sSf https://raw.githubusercontent.com/leanprover/elan/master/elan-init.sh | sh -s -- -y
        echo "$HOME/.elan/bin" >> $GITHUB_PATH
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        
    - name: Test Lean verification performance
      run: |
        pytest tests/performance/test_lean_performance.py -v \
          --benchmark-json=lean-benchmark.json \
          --timeout=1800  # 30 minute timeout
          
    - name: Test proof complexity scaling
      run: |
        python tests/performance/proof_complexity_scaling.py > proof-scaling-results.txt
        
    - name: Upload proof assistant results
      uses: actions/upload-artifact@v3
      with:
        name: proof-assistant-performance
        path: |
          lean-benchmark.json
          proof-scaling-results.txt

  performance-report:
    name: Generate Performance Report
    runs-on: ubuntu-latest
    needs: [basic-benchmarks, comprehensive-performance, load-testing, memory-profiling, llm-performance, proof-assistant-performance]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all performance artifacts
      uses: actions/download-artifact@v3
      
    - name: Generate comprehensive report
      run: |
        python scripts/generate_performance_report.py \
          --output performance-report.html \
          --json-output performance-summary.json \
          --artifacts-dir .
          
    - name: Generate performance trends
      run: |
        # Compare with historical data if available
        if [ -f historical-performance.json ]; then
          python scripts/analyze_performance_trends.py \
            historical-performance.json \
            performance-summary.json \
            --output trends-report.html
        fi
        
    - name: Upload performance report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report
        path: |
          performance-report.html
          performance-summary.json
          trends-report.html
          
    - name: Comment performance summary on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          if (fs.existsSync('performance-summary.json')) {
            const summary = JSON.parse(fs.readFileSync('performance-summary.json', 'utf8'));
            
            const comment = `## 📊 Performance Test Results
            
            ### Summary
            - **Basic benchmarks**: ${summary.basic_benchmarks || 'N/A'}
            - **Memory usage**: ${summary.peak_memory || 'N/A'} MB
            - **LLM API average response**: ${summary.llm_avg_response || 'N/A'} ms
            - **Proof verification time**: ${summary.proof_avg_time || 'N/A'} ms
            
            ### Key Metrics
            ${summary.regressions ? '⚠️ Performance regressions detected!' : '✅ No significant regressions'}
            
            [View detailed report](${summary.report_url || '#'})
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }

  performance-regression-check:
    name: Performance Regression Check
    runs-on: ubuntu-latest
    needs: [basic-benchmarks]
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download benchmark results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results-basic
        
    - name: Check for performance regressions
      run: |
        # Download baseline from main branch
        git fetch origin main
        git checkout origin/main -- baseline-benchmarks.json || echo "No baseline found"
        
        if [ -f baseline-benchmarks.json ]; then
          python scripts/check_performance_regression.py \
            baseline-benchmarks.json \
            benchmark-results.json \
            --threshold=15 \
            --fail-on-regression
        else
          echo "No baseline benchmarks found, skipping regression check"
        fi