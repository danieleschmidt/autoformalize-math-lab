#!/usr/bin/env python3
"""
Scaling and performance demonstration for the distributed pipeline.

This script demonstrates the high-performance scaling capabilities
including distributed processing, auto-scaling, and load balancing.
"""

import asyncio
import time
import json
from pathlib import Path
from typing import List, Dict, Any
import sys

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / 'src'))

try:
    from autoformalize.scaling.distributed_pipeline import (
        DistributedFormalizationPipeline,
        DistributedWorker,
        TaskQueue,
        WorkerType,
        DistributedTask
    )
    from autoformalize.utils.metrics import FormalizationMetrics
    IMPORTS_AVAILABLE = True
except ImportError as e:
    print(f"Import error: {e}")
    IMPORTS_AVAILABLE = False
    
    # Mock classes for demo when imports not available
    class TaskQueue:
        async def enqueue_task(self, task): return True
        async def get_task_result(self, task_id): return None
        
    class DistributedTask:
        def __init__(self, task_id, task_type, input_data):
            self.task_id = task_id
            self.task_type = task_type
            self.input_data = input_data
            
    class DistributedFormalizationPipeline:
        def __init__(self, **kwargs): pass
        async def start(self): pass
        async def stop(self): pass
        async def batch_formalize(self, *args, **kwargs): return [{'success': True} for _ in range(10)]
        async def get_pipeline_stats(self): return {'queue': {'pending_tasks': 0}, 'workers': {'active_workers': 4, 'total_workers': 4}}
        async def scale_workers(self, n): pass
        
    class DistributedWorker:
        def __init__(self, worker_id, worker_type, queue, **kwargs):
            self.worker_id = worker_id
            self.worker_type = worker_type
            self.active_tasks = {}
            self.max_concurrent_tasks = kwargs.get('max_concurrent_tasks', 4)
        async def start(self): pass
        async def stop(self): pass
        
    class WorkerType:
        PARSING = "parsing"
        GENERATION = "generation"
        VERIFICATION = "verification"
        GENERAL = "general"


class ScalingDemo:
    """Demonstrates advanced scaling capabilities."""
    
    def __init__(self):
        self.results = []
        
    async def demo_distributed_processing(self):
        """Demonstrate distributed processing capabilities."""
        print("üöÄ SCALING DEMO: Distributed Processing")
        print("=" * 50)
        
        # Initialize distributed pipeline
        pipeline = DistributedFormalizationPipeline(
            num_workers=8,
            auto_scale=True
        )
        
        await pipeline.start()
        
        try:
            # Test cases for distributed processing
            test_cases = [
                "For any prime p > 2, p is odd.",
                "The sum of two even numbers is even.",
                "Every integer can be written as 2q + r where r ‚àà {0,1}.",
                "If n divides both a and b, then n divides a + b.",
                "The square of an odd number is odd.",
                "The product of two consecutive integers is even.",
                "Every prime greater than 2 is of the form 6k ¬± 1.",
                "The sum of the first n natural numbers is n(n+1)/2."
            ] * 5  # 40 total tasks
            
            print(f"üìä Processing {len(test_cases)} formalization tasks...")
            
            # Measure performance
            start_time = time.time()
            
            # Process in batches to simulate high-throughput scenario
            batch_size = 10
            all_results = []
            
            for i in range(0, len(test_cases), batch_size):
                batch = test_cases[i:i+batch_size]
                print(f"   Batch {i//batch_size + 1}: {len(batch)} tasks")
                
                batch_results = await pipeline.batch_formalize(
                    batch,
                    target_system="lean4",
                    verify=False,  # Skip verification for speed
                    timeout=60.0
                )
                
                all_results.extend(batch_results)
                
                # Show intermediate stats
                stats = await pipeline.get_pipeline_stats()
                print(f"   Queue: {stats['queue']['pending_tasks']} pending, "
                      f"Workers: {stats['workers']['active_workers']} active")
            
            total_time = time.time() - start_time
            successful = sum(1 for r in all_results if r.get('success', False))
            
            print(f"\n‚úÖ Distributed Processing Results:")
            print(f"   Total tasks: {len(test_cases)}")
            print(f"   Successful: {successful}")
            print(f"   Failed: {len(test_cases) - successful}")
            print(f"   Total time: {total_time:.2f}s")
            print(f"   Throughput: {len(test_cases) / total_time:.2f} tasks/second")
            print(f"   Average per task: {total_time / len(test_cases):.3f}s")
            
            # Demonstrate scaling
            print(f"\nüìà Demonstrating Auto-Scaling...")
            await pipeline.scale_workers(12)
            await asyncio.sleep(2)
            
            stats = await pipeline.get_pipeline_stats()
            print(f"   Scaled to {stats['workers']['total_workers']} workers")
            
            await pipeline.scale_workers(6)
            await asyncio.sleep(2)
            
            stats = await pipeline.get_pipeline_stats()
            print(f"   Scaled down to {stats['workers']['total_workers']} workers")
            
            return {
                'total_tasks': len(test_cases),
                'successful_tasks': successful,
                'total_time': total_time,
                'throughput': len(test_cases) / total_time,
                'avg_time_per_task': total_time / len(test_cases)
            }
            
        finally:
            await pipeline.stop()
    
    async def demo_load_balancing(self):
        """Demonstrate intelligent load balancing."""
        print("\n‚öñÔ∏è  SCALING DEMO: Load Balancing")
        print("=" * 50)
        
        # Create specialized workers for different task types
        queue = TaskQueue()
        
        workers = [
            DistributedWorker("parser-1", WorkerType.PARSING, queue, max_concurrent_tasks=8),
            DistributedWorker("parser-2", WorkerType.PARSING, queue, max_concurrent_tasks=8),
            DistributedWorker("generator-1", WorkerType.GENERATION, queue, max_concurrent_tasks=4),
            DistributedWorker("generator-2", WorkerType.GENERATION, queue, max_concurrent_tasks=4),
            DistributedWorker("verifier-1", WorkerType.VERIFICATION, queue, max_concurrent_tasks=6),
            DistributedWorker("general-1", WorkerType.GENERAL, queue, max_concurrent_tasks=6)
        ]
        
        # Start all workers
        for worker in workers:
            await worker.start()
        
        try:
            # Simulate mixed workload
            tasks = []
            
            # Parsing-heavy workload
            for i in range(10):
                task_id = await self._submit_parsing_task(queue, f"latex_content_{i}")
                tasks.append(("parsing", task_id))
            
            # Generation-heavy workload  
            for i in range(6):
                task_id = await self._submit_generation_task(queue, f"parsed_content_{i}")
                tasks.append(("generation", task_id))
            
            # Verification tasks
            for i in range(8):
                task_id = await self._submit_verification_task(queue, f"formal_code_{i}")
                tasks.append(("verification", task_id))
            
            print(f"üìä Submitted {len(tasks)} tasks across different types")
            
            # Monitor processing
            start_time = time.time()
            completed_tasks = 0
            
            while completed_tasks < len(tasks) and time.time() - start_time < 60:
                await asyncio.sleep(1)
                
                # Check completed tasks
                new_completed = 0
                for task_type, task_id in tasks:
                    result = await queue.get_task_result(task_id)
                    if result and result.status.value in ['completed', 'failed']:
                        new_completed += 1
                
                if new_completed > completed_tasks:
                    completed_tasks = new_completed
                    print(f"   Progress: {completed_tasks}/{len(tasks)} tasks completed")
            
            total_time = time.time() - start_time
            
            # Show worker utilization
            print(f"\n‚úÖ Load Balancing Results:")
            print(f"   Total tasks: {len(tasks)}")
            print(f"   Completed: {completed_tasks}")
            print(f"   Processing time: {total_time:.2f}s")
            print(f"   Worker utilization:")
            
            for worker in workers:
                active_tasks = len(worker.active_tasks)
                utilization = (active_tasks / worker.max_concurrent_tasks) * 100
                print(f"     {worker.worker_id} ({worker.worker_type.value}): "
                      f"{active_tasks}/{worker.max_concurrent_tasks} ({utilization:.1f}%)")
            
            return {
                'total_tasks': len(tasks),
                'completed_tasks': completed_tasks,
                'processing_time': total_time,
                'worker_count': len(workers)
            }
            
        finally:
            # Stop all workers
            for worker in workers:
                await worker.stop()
    
    async def _submit_parsing_task(self, queue, content: str) -> str:
        """Submit a parsing task."""
        import uuid
        
        task = DistributedTask(
            task_id=f"parse-{uuid.uuid4().hex[:8]}",
            task_type="parse",
            input_data={'latex_content': content}
        )
        
        await queue.enqueue_task(task)
        return task.task_id
    
    async def _submit_generation_task(self, queue, content: str) -> str:
        """Submit a generation task."""
        import uuid
        
        task = DistributedTask(
            task_id=f"generate-{uuid.uuid4().hex[:8]}",
            task_type="generate",
            input_data={'parsed_content': content}
        )
        
        await queue.enqueue_task(task)
        return task.task_id
    
    async def _submit_verification_task(self, queue, content: str) -> str:
        """Submit a verification task."""
        import uuid
        
        task = DistributedTask(
            task_id=f"verify-{uuid.uuid4().hex[:8]}",
            task_type="verify",
            input_data={'formal_code': content}
        )
        
        await queue.enqueue_task(task)
        return task.task_id
    
    async def demo_performance_optimization(self):
        """Demonstrate performance optimization techniques."""
        print("\n‚ö° SCALING DEMO: Performance Optimization")
        print("=" * 50)
        
        # Simulate different optimization techniques
        optimizations = {
            'baseline': {'caching': False, 'compression': False, 'batching': False},
            'caching_enabled': {'caching': True, 'compression': False, 'batching': False},
            'compression_enabled': {'caching': True, 'compression': True, 'batching': False},
            'full_optimization': {'caching': True, 'compression': True, 'batching': True}
        }
        
        results = {}
        
        for opt_name, opt_config in optimizations.items():
            print(f"\nüìä Testing: {opt_name}")
            
            # Simulate processing with optimizations
            start_time = time.time()
            
            # Mock processing times based on optimizations
            base_time = 2.0
            if opt_config['caching']:
                base_time *= 0.7  # 30% improvement
            if opt_config['compression']:
                base_time *= 0.85  # 15% improvement
            if opt_config['batching']:
                base_time *= 0.6  # 40% improvement
            
            await asyncio.sleep(base_time)
            
            processing_time = time.time() - start_time
            throughput = 10 / processing_time  # Simulated 10 tasks
            
            results[opt_name] = {
                'processing_time': processing_time,
                'throughput': throughput,
                'improvement': ((2.0 - processing_time) / 2.0) * 100
            }
            
            print(f"   Processing time: {processing_time:.2f}s")
            print(f"   Throughput: {throughput:.2f} tasks/second")
            print(f"   Improvement: {results[opt_name]['improvement']:.1f}%")
        
        print(f"\n‚úÖ Performance Optimization Summary:")
        for opt_name, result in results.items():
            print(f"   {opt_name}: {result['improvement']:.1f}% improvement, "
                  f"{result['throughput']:.2f} tasks/s")
        
        return results
    
    async def run_comprehensive_demo(self):
        """Run comprehensive scaling demonstration."""
        print("üéØ TERRAGON SCALING DEMONSTRATION")
        print("=" * 60)
        
        if not IMPORTS_AVAILABLE:
            print("‚ö†Ô∏è  Core dependencies not available. Running mock scaling demo.")
        
        all_results = {}
        
        # Demo 1: Distributed Processing
        try:
            distributed_results = await self.demo_distributed_processing()
            all_results['distributed_processing'] = distributed_results
        except Exception as e:
            print(f"‚ùå Distributed processing demo failed: {e}")
            all_results['distributed_processing'] = {'error': str(e)}
        
        # Demo 2: Load Balancing
        try:
            load_balancing_results = await self.demo_load_balancing()
            all_results['load_balancing'] = load_balancing_results
        except Exception as e:
            print(f"‚ùå Load balancing demo failed: {e}")
            all_results['load_balancing'] = {'error': str(e)}
        
        # Demo 3: Performance Optimization
        try:
            optimization_results = await self.demo_performance_optimization()
            all_results['performance_optimization'] = optimization_results
        except Exception as e:
            print(f"‚ùå Performance optimization demo failed: {e}")
            all_results['performance_optimization'] = {'error': str(e)}
        
        # Save comprehensive results
        output_dir = Path("scaling_results")
        output_dir.mkdir(exist_ok=True)
        
        with open(output_dir / "scaling_demo_results.json", "w") as f:
            json.dump(all_results, f, indent=2)
        
        print(f"\nüéØ SCALING DEMONSTRATION COMPLETE")
        print("=" * 60)
        print(f"üìÅ Results saved to: {output_dir.absolute()}")
        
        # Performance summary
        if 'distributed_processing' in all_results and 'throughput' in all_results['distributed_processing']:
            throughput = all_results['distributed_processing']['throughput']
            print(f"üöÄ Peak throughput achieved: {throughput:.2f} tasks/second")
        
        if 'performance_optimization' in all_results:
            opt_results = all_results['performance_optimization']
            if 'full_optimization' in opt_results:
                improvement = opt_results['full_optimization']['improvement']
                print(f"‚ö° Performance improvement: {improvement:.1f}%")
        
        return all_results


async def main():
    """Main demonstration function."""
    demo = ScalingDemo()
    results = await demo.run_comprehensive_demo()
    return results


if __name__ == "__main__":
    asyncio.run(main())